# -*- coding: utf-8 -*-
'''
General tools for support of MOSCA's functionalities

By JoÃ£o Sequeira

Jun 2017
'''

import glob
from pathlib import Path
import shutil

import numpy as np
import os
import pandas as pd
from subprocess import run, Popen, PIPE, check_output
import sys
import time
from tqdm import tqdm

blast_cols = ['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen', 'qstart', 'qend', 'sstart', 'send',
              'evalue', 'bitscore']


def run_command(bashCommand, output='', mode='w', sep=' ', print_message=True, verbose=True):
    if print_message:
        print(f"{bashCommand.replace(sep, ' ')}{' > ' + output if output != '' else ''}")
    if output == '':
        run(bashCommand.split(sep), stdout=sys.stdout if verbose else None, check=True)
    else:
        with open(output, mode) as output_file:
            run(bashCommand.split(sep), stdout=output_file)


def run_pipe_command(bashCommand, output='', mode='w', sep=' ', print_message=True):
    if print_message:
        print(bashCommand)
    if output == '':
        Popen(bashCommand, stdin=PIPE, shell=True).communicate()
    elif output == 'PIPE':
        return Popen(bashCommand, stdin=PIPE, shell=True, stdout=PIPE).communicate()[0].decode('utf8')
    else:
        with open(output, mode) as output_file:
            Popen(bashCommand, stdin=PIPE, shell=True, stdout=output_file).communicate()


def parse_blast(blast):
    result = pd.read_csv(blast, sep='\t', header=None)
    result.columns = blast_cols
    return result


def check_bowtie2_index(index_prefix):
    files = glob.glob(index_prefix + '*.bt2')
    if len(files) < 6:
        return False
    return True


def generate_mg_index(reference, index_prefix):
    run_pipe_command(f'bowtie2-build {reference} {index_prefix} 1> {index_prefix}.log 2> {index_prefix}.err')


def align_reads(reads, index_prefix, sam, report, log=None, threads=6):
    run_pipe_command(
        f'bowtie2 -x {index_prefix} -1 {reads[0]} -2 {reads[1]} -S {sam} -p {threads} 1> {report} 2> {log}')


def parse_fasta(file):
    print(f'Parsing {file}')
    lines = [line.rstrip('\n') for line in open(file)]
    i = 0
    sequences = dict()
    while i < len(lines):
        if lines[i].startswith('>'):
            name = lines[i][1:]
            sequences[name] = ''
            i += 1
            while i < len(lines) and not lines[i].startswith('>'):
                sequences[name] += lines[i]
                i += 1
    return sequences


def build_gff_from_contigs(contigs, output, assembler=None):
    gff = pd.DataFrame()
    contigs = parse_fasta(contigs)
    contigs = [[k, v] for k, v in contigs.items()]
    gff["seqid"] = [contig[0] for contig in contigs]
    size = len(contigs)
    gff["source"] = ['.' if assembler is None else assembler] * size
    gff["type"] = ['exon'] * size
    gff["start"] = ['1'] * size
    gff["end"] = [len(contigs[1]) for contigs in contigs]
    gff["score"] = ['.'] * size
    gff["strand"] = ['.'] * size
    gff["phase"] = ['.'] * size
    gff["attributes"] = ['gene_id=' + contig[0] for contig in contigs]
    gff.to_csv(output, sep='\t', index=False, header=False)


def build_gff_from_orfs(orfs, output):
    gff = pd.DataFrame()
    qseqids = parse_fasta(orfs).keys()
    parts = [qid.split('_') for qid in qseqids]
    gff["seqid"] = qseqids
    size = gff.size
    gff["source"] = ['UniProtKB'] * size
    gff["type"] = ['exon'] * size
    gff["start"] = [1] * size
    gff["end"] = [int(part[-2]) - int(part[-3]) + 1 for part in parts]
    gff["score"] = ['.'] * size
    gff["strand"] = [part[-1] for part in parts]
    gff["phase"] = ['.'] * size
    gff["Name"] = [f"Name={'_'.join(part)}" for part in parts]
    gff.to_csv(output, sep='\t', index=False, header=False)


'''
Input:
    reference: name of contigs file from assembly
    reads: list, [forward reads, reverse reads]
    basename: basename of outputs
    threads: number of threads to use
    attribute: str - identifier to group quantification for
    type_of_reference: str - 'contigs' or 'orfs'
Output:
    Will generate a bowtie2 index named contigs.replace(.fasta,_index), 
    GFF annotation file named blast.replace(.blast,.gff)
    SAM alignment and READCOUNTS files named output + .sam and .readcounts
'''


def perform_alignment(reference, reads, basename, threads=1):
    ext = f".{reference.split('.')[-1]}"
    if not check_bowtie2_index(reference.replace(ext, '_index')):
        print('INDEX files not found. Generating new ones')
        generate_mg_index(reference, reference.replace(ext, '_index'))
    else:
        print(f"INDEX was located at {reference.replace(ext, '_index')}")
    if not os.path.isfile(f'{basename}.log'):
        align_reads(reads, reference.replace(ext, '_index'), f'{basename}.sam',
                    f'{basename}_bowtie2_report.txt', log=f'{basename}.log', threads=threads)
    else:
        print(f'{basename}.log was found!')
    run_pipe_command(
        f"""samtools view -F 260 -S {basename}.sam | cut -f 3 | sort | uniq -c | 
        awk '{{printf("%s\\t%s\\n", $2, $1)}}'""", output=f'{basename}.readcounts')


def fastq2fasta(fastq, output):
    run_command(f"""paste - - - - < {fastq} | cut -f 1,2 | sed 's/^@/>/' | tr \"\t\" \"\n\" > {output}""")


def timed_message(message):
    print(time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime()) + ': ' + message)


def normalize_mg_by_size(readcounts, contigs): ##falar com Joao, precisar de criar os normalized readcounts, pois ele esta a alterar as de input.
    from pathlib import Path
    if os.path.exists('output/Quantification/') == False:
        os.mkdir('output/Quantification/')
    shutil.copy(readcounts,'output/Quantification/')
    name = Path(readcounts).name
    readcounts = f'output/Quantification/{name}'
    print(readcounts)
    run_pipe_command(
        f"seqkit fx2tab {contigs} | sort | awk '{{print $1\"\\t\"length($2)}}' | "
        f"join - {readcounts} | awk '{{print $1\"\\t\"$3/$2}}'",
        output=readcounts.replace('.readcounts', '_normalized.readcounts'))


'''
Input: 
    data: pd.DataFrame - Protein Report on the making
    readcounts: readcounts file from quantification with htseq-count
    origin_of_data: 'metagenomics', 'metatranscriptomics' defines column of joining
    name: name of column to add to relation
Output: 
    'data' df will receive additional column with abundance/expression 
    information. This column will be named 'name'
'''


def add_abundance(data, readcounts, name, origin_of_data='metagenomics'):
    readcounts = pd.read_csv(readcounts, sep='\t', header=None, names=['qseqid', name])
    readcounts[name] = readcounts[name].fillna(value=0)
    if origin_of_data == 'metagenomics':
        readcounts['Contig'] = [qseqid.split('_')[1] for qseqid in readcounts['qseqid']]
        del readcounts['qseqid']
        return pd.merge(data, readcounts, on='Contig', how='left')
    elif origin_of_data == 'metatranscriptomics':
        return pd.merge(data, readcounts, on='qseqid', how='left')
    elif origin_of_data == 'compomics':
        pass


def multi_sheet_excel(output, data, sheet_name='Sheet', lines=1000000, index=False):
    writer = pd.ExcelWriter(output, engine='xlsxwriter')
    if len(data) < lines:
        data.to_excel(writer, sheet_name=f'{sheet_name}', index=index)
    else:
        for i in range(0, len(data), lines):
            j = min(i + lines, len(data))
            data.iloc[i:(i + lines)].to_excel(writer, sheet_name=f'{sheet_name} ({j})', index=index)
    writer.save()


'''
Input:
    joined: name of final TSV file outputed by MOSCA
    columns: name of columns to normalize (don't put them all at once, one
    normalization at a time!)
    output: name of file to output
Output:
    A TXT file will be generated with a single column of numbers, each one
    corresponding to the normalization factor of each sample by order of
    column in the readcounts file
'''


def normalize_readcounts(joined, columns, method='TMM'):
    working_dir = '/'.join(joined.split('/')[:-1])
    info = pd.read_csv(joined, sep='\t')
    info[columns] = info[columns].fillna(value=0)
    info[columns].to_csv(working_dir + '/to_normalize.tsv', sep='\t', index=False)
    print(f"Normalizing {joined} on columns {','.join(columns)}")
    run_command(
        f'Rscript {sys.path[0]}/normalization.R --readcounts {working_dir}/to_normalize.tsv '
        f'--output {working_dir}/normalization_factors.txt -m {method}')
    factors = open(f'{working_dir}/normalization_factors.txt').read().split('\n')[:-1]  # \n always last element

    for i in range(len(columns)):
        info[f'{columns[i]}_normalized'] = info[columns[i]] * float(factors[i])
    return info


def timed_message(message=None):
    print(f'{time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())}: {message}')


def expand_by_list_column(df, column='Pathway'):
    lens = [len(item) for item in df[column]]
    dictionary = dict()
    for column in df.columns:
        dictionary[column] = np.repeat(df[column].values, lens)
    dictionary[column] = np.concatenate(df[column].values)
    return pd.DataFrame(dictionary)


def parse_fastqc_report(filename):
    data = dict()
    file = open(filename).read().split('\n')
    i = 1
    while i < len(file):
        if file[i].startswith('>>') and file[i] != '>>END_MODULE':
            name, flag = file[i][2:].split('\t')[0], file[i][2:].split('\t')[1]
            if name == 'Sequence Duplication Levels':
                i += 1
            i += 1
            if file[i] == '>>END_MODULE':
                data[name] = (flag, pd.DataFrame())
            else:
                labels = file[i][1:].split('\t')
                i += 1
                partial_data = np.array(labels)
                while i < len(file) and not file[i].startswith('>>'):
                    partial_data = np.append(partial_data, file[i].split('\t'))
                    i += 1
                partial_data = np.reshape(partial_data, (int(partial_data.size / len(labels)), len(labels)))
                data[name] = (flag, pd.DataFrame(data=partial_data[1:, 1:], index=partial_data[1:, 0],
                                                 columns=partial_data[0, 1:]))
        i += 1
    return data


def count_on_file(expression, file, compressed=False):
    return int(check_output(f"{'zgrep' if compressed else 'grep'} -c '{expression}' {file}", shell=True))


def sort_alphanumeric(alphanumeric_list):
    return sorted(alphanumeric_list, key=lambda item: (
        int(item.partition(' ')[0]) if item[0].isdigit() else float('inf'), item))


def generate_expression_matrix(readcount_files, header, output):
    expression_matrix = pd.DataFrame()
    for file in readcount_files:
        df = pd.read_csv(file, sep='\t', index_col=0, header=None)
        df.columns = [file.split('/')[-1].rstrip('.readcounts')]
        expression_matrix = pd.merge(expression_matrix, df, how='outer',
                                     left_index=True, right_index=True)
    expression_matrix = expression_matrix[1:-5]  # remove non identified proteins (*) and the metrics at the end
    expression_matrix = expression_matrix.fillna(value=0).astype(
        int)  # set not identified proteins expression to 0, and convert floats, since DeSEQ2 only accepts integers
    expression_matrix.index.name = 'geneid'
    expression_matrix.columns = header
    expression_matrix.to_csv(output, sep='\t')


def make_protein_report(Upimapi, reCognizer, readcounts_input, exps, sample, contigs):
    timed_message(f'Joining data for sample: {sample}')
    report = pd.read_csv(reCognizer, sep='\t')
    report = report.groupby('qseqid')[report.columns.tolist()[1:]].first().reset_index()
    report = report[report['DB ID'].str.startswith('COG')].rename(columns={'DB ID': 'COG ID'})
    report = pd.merge(pd.read_csv(Upimapi, sep='\t'), report, on='qseqid')
    report = report.rename(columns={**{f'{col}_x': f'{col} (UPIMAPI)' for col in blast_cols},
                                    **{f'{col}_y': f'{col} (reCOGnizer)' for col in blast_cols}})
    report['Contig'] = report['qseqid'].apply(lambda x: x.split('_')[1])
    mg_names = exps[(exps['Sample'] == sample) & (exps['Data type'] == 'dna')]['Name'].tolist()
    mt_names = exps[(exps['Sample'] == sample) & (exps['Data type'] == 'mrna')]['Name'].tolist()
    for mg_name in mg_names:
        for i in readcounts_input:
                if i == f"input/{mg_name}.readcounts" or i == f"output/Quantification/{mg_name}.readcounts":
                    mg_we_need = i
        readcounts = pd.read_csv(
            mg_we_need, sep='\t', header=None,
            names=['Contig', mg_name])
        normalize_mg_by_size(
            mg_we_need, contigs)
        norm_by_size = pd.read_csv(
            f'output/Quantification/{mg_name}_normalized.readcounts', sep='\t', header=None,
            names=['Contig', f'{mg_name} (Normalized by contig size)'])
        for counts in [readcounts, norm_by_size]:
            counts['Contig'] = counts['Contig'].apply(lambda x: x.split('_')[1])
        report = pd.merge(report, readcounts, on='Contig', how='left')
        report = pd.merge(report, norm_by_size, on='Contig', how='left')
    for mt_name in mt_names:
        for i in readcounts_input:
                if i == f"input/{mt_name}.readcounts" or i == f"output/Quantification/{mt_name}.readcounts":
                    mt_we_need = i
        readcounts = pd.read_csv(mt_we_need, sep='\t', header=None,
                                    names=['qseqid', mt_name])
        report = pd.merge(report, readcounts, on='qseqid', how='left')
    report[mg_names + mt_names] = report[mg_names + mt_names].fillna(value=0).astype(int)
    report[[f'{name} (Normalized by contig size)' for name in mg_names]] = report[
        [f'{name} (Normalized by contig size)' for name in mg_names]].fillna(value=0)
    multi_sheet_excel('output/MOSCA_Protein_Report.xlsx', report, sheet_name=sample)


def make_entry_report(protein_report, out, exps, Upimapi_input):
    for sample in set(exps['Sample']):
        for i in Upimapi_input:
                if i == f"input/{sample}_UPIMAPI_results.tsv" or i == f"output/Annotation/{sample}/UPIMAPI_results.tsv":
                    Upimapi = i
        report = pd.read_excel(protein_report, sheet_name=sample)
        upimapi_res = pd.read_csv(Upimapi, sep='\t')
        uniprot_cols = [col for col in upimapi_res.columns if col not in blast_cols]
        taxonomy_columns = [col for col in upimapi_res.columns if 'Taxonomic lineage (' in col]
        functional_columns = [
            'COG general functional category', 'COG functional category', 'Protein description', 'cog']
        print(f'Finding consensus COG for each entry of sample: {sample}')
        tqdm.pandas()
        cogs_df = report.groupby('Entry')['cog'].progress_apply(
            lambda x: x.value_counts().index[0] if len(x.value_counts().index) > 0 else np.nan).reset_index()
        cogs_categories = report[functional_columns].drop_duplicates()
        mg_names = exps[(exps["Data type"] == 'dna') & (exps["Sample"] == sample)]['Name'].tolist()
        mt_names = exps[(exps["Data type"] == 'mrna') & (exps["Sample"] == sample)]['Name'].tolist()
        # Aggregate information for each Entry, keep UniProt information, sum MG and MT or MP quantification
        if len(mg_names) > 0:
            report.rename(
                columns={f'{mg_name} (Normalized by contig size)': mg_name for mg_name in mg_names}, inplace=True)
        report = report.groupby('Entry')[mg_names + mt_names].sum().reset_index()
        report = pd.merge(report, upimapi_res, on='Entry', how='left')
        report = pd.merge(report, cogs_df, on='Entry', how='left')
        report = pd.merge(report, cogs_categories, on='cog', how='left')
        report = report[uniprot_cols + functional_columns + mg_names + mt_names]
        # MG normalization by sample and protein abundance
        if len(mg_names) > 0:
            report[mg_names].to_csv(f'{out}/Quantification/mg_preprocessed_readcounts.tsv', sep='\t', index=False)
            report = pd.concat([report, normalize_readcounts(
                f'{out}/Quantification/mg_preprocessed_readcounts.tsv', mg_names)[
                [f'{col}_normalized' for col in mg_names]]], axis=1)
        # MT normalization by sample and protein expression
        if len(mt_names) > 0:
            report[mt_names].to_csv(f'{out}/Quantification/expression_analysed_readcounts.tsv', sep='\t', index=False)
            report = pd.concat([report, normalize_readcounts(
                f'{out}/Quantification/expression_analysed_readcounts.tsv', mt_names)[
                [f'{col}_normalized' for col in mt_names]]], axis=1)
        # For each sample, write an Entry Report
        report = report.drop_duplicates()
        timed_message('Writing Entry Report.')
        multi_sheet_excel(f'{out}/MOSCA_Entry_Report.xlsx', report, sheet_name=sample)
        # Also make expression matrix for DE analysis
        timed_message('Written expression matrix.')
        if len(mt_names) > 0:
            Path(f'{out}/Quantification/{sample}').mkdir(parents=True, exist_ok=True)
            report[['Entry'] + mt_names].groupby('Entry')[mt_names].sum().reset_index().to_csv(
                f'{out}/Quantification/{sample}/expression_matrix.tsv', sep='\t', index=False)
        if len(mg_names) == 0:
            mg_names = mt_names
        for mg_name in mg_names:
            # Draw the taxonomy krona plot
            report.groupby(taxonomy_columns)[mg_name].sum().reset_index()[[mg_name] + taxonomy_columns].to_csv(
                f'{out}/{mg_name}_tax.tsv', sep='\t', index=False, header=False)
            run_command('ktImportText {0}/{1}_tax.tsv -o {0}/{1}_tax.html'.format(out, mg_name))
            # Draw the functional krona plot
            report.groupby(functional_columns)[mg_name].sum().reset_index()[[mg_name] + functional_columns].to_csv(
                f'{out}/{mg_name}_fun.tsv', sep='\t', index=False, header=False)
            run_command('ktImportText {0}/{1}_fun.tsv -o {0}/{1}_fun.html'.format(out, mg_name))


def fastqc_name(filename):
    return filename.replace("stdin:", "").replace(".gz", "").replace(".bz2", "").replace(".txt", "").replace(
        ".fastq", "").replace(".fq", "").replace(".csfastq", "").replace(".sam", "").replace(".bam", "")